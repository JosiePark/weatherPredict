{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d892db6e",
   "metadata": {},
   "source": [
    "# Predicting the daily average temperature\n",
    "\n",
    "We will build a couple of simple POC models to predict the daily average temperature in California using data from the years 2019 and 2020. For the purpose of this study, we will just focus on univariate time series prediction.\n",
    "\n",
    "A lot of business rely on weather data to make decisions. For example, insurance companies may use weather data to evaluate the probability and cost of insurance claims, or supermarkets may use weather to help decide what to stock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfda736",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d1b2c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from pmdarima.arima import auto_arima\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from neuralprophet import NeuralProphet\n",
    "\n",
    "from weatherPredict.ts_funcs import stationary_test\n",
    "from weatherPredict.plotting import ARIMA_predict_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816f3771",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data\\\\weather_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweather_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnnamed: 0\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\weather_env\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\weather_env\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\weather_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\weather_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\weather_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\weather_env\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\weather_env\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data\\\\weather_data.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(\"data\", \"weather_data.csv\")).drop(columns = ['Unnamed: 0'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eb2a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c1d6bb",
   "metadata": {},
   "source": [
    "We focus on the columns that we are interested in. For now, let's look at the temperature columns and precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0933b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process\n",
    "cols_to_keep = [\n",
    "    'datetime',\n",
    "    'tempmax',\n",
    "    'tempmin',\n",
    "    'temp',\n",
    "    'precip',\n",
    "]\n",
    "\n",
    "df = df[cols_to_keep]\n",
    "df['datetime'] = pd.to_datetime(df.loc[:,'datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea5c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check for missing values\n",
    "df.isnull().values.any()\n",
    "df.isnull().sum()\n",
    "# there are none so no need to fill entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde54055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's construct some smooth data so it's easier to visualise\n",
    "df['temp_7day_MA'] = df['temp'].rolling(window = 7).mean()\n",
    "df['tempmin_7day_MA'] = df['tempmin'].rolling(window = 7).mean()\n",
    "df['tempmax_7day_MA'] = df['tempmax'].rolling(window = 7).mean()\n",
    "df['precip_7day_sum'] = df['precip'].rolling(window = 7).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d309cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x = 'datetime', y = ['temp', 'temp_7day_MA'])\n",
    "plt.grid();\n",
    "plt.ylim([0,35]);\n",
    "plt.title('Daily average temperature in California');\n",
    "plt.ylabel('Temperature (degrees Celsius)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a314079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.fill_between(df['datetime'], df['tempmin_7day_MA'], df['tempmax_7day_MA'], alpha = 0.3)\n",
    "plt.title(\"Daily minimum and maximum temperature ranges in California\");\n",
    "plt.ylabel('Temperature (degrees Celsius)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed72028",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot('datetime', 'precip_7day_sum')\n",
    "plt.title('Weekly total precipitation in California')\n",
    "plt.ylabel('Weekly precipitation (mm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7106c57",
   "metadata": {},
   "source": [
    "We see that there is unsurprisinly yearly seasoanality in temperature trends. The daily temperature range remains relatively constant as a proportion of average temperature, and so we will not attempt to predict either of those. Instead we will focus on the daily average tempperature.\n",
    "\n",
    "Furthermore, it appears there is little regular rain in California. In other words, it rains infrequently. There does appear to be more rain around January than in other times of the year, but there isn't much of a trend. Therefore, it also does not seem appropriate to model precipitation using time series analysis techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3201a651",
   "metadata": {},
   "source": [
    "## Time Series Analysis\n",
    "\n",
    "We will explore some of the properties of the time series data, and also explore how to transform the data so it is suitable for an ARIMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75827100",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('datetime', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be445f5",
   "metadata": {},
   "source": [
    "We can use the statsmodels seasonal decompose function to decompose the time series into a trend and seasonal component, and what the leftover residual is.\n",
    "\n",
    "Seasonal decompose is not picking up the annual seasonality. However, we would likely need more than two years of data to do this. As a result, the overall trend is still quite noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_data = seasonal_decompose(df['temp'])\n",
    "fig = decomposed_data.plot();\n",
    "fig.set_size_inches((16, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d83b31",
   "metadata": {},
   "source": [
    "ARIMA models require the data to be stationary. Therefore, we perform the Augmented Dicker-Fuller test to test for stationarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbcfea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate stationarity\n",
    "stationary_test(df, 'temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f990e322",
   "metadata": {},
   "source": [
    "Our threshold p-value is 0.05. As the p-value is greater than 0.05, we fail to reject the null hypothesis that the time-series is non-stationary.\n",
    "\n",
    "Taking the log of a time series analysis is one way to reduce the deviation from the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744b4609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is not stationary\n",
    "df['log_temp'] = np.log(df['temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cfd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(y = 'log_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adab92bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate stationarity\n",
    "stationary_test(df, 'log_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aee17f6",
   "metadata": {},
   "source": [
    "It is still non-stationary.\n",
    "\n",
    "The d-order of an ARIMA model is the order of differencing used. We want to find the smallest number d such that our time series is stationary. Let's take the first order difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ce917",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diff_log_temp'] = df['log_temp'].diff()\n",
    "stationary_test(df.dropna(), 'diff_log_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec501583",
   "metadata": {},
   "source": [
    "Now our p-value < 0.05, so we can reject the null hypothesis, and infer that the time series is now stationary. We plot the difference of the log of the temp below so we can visually verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4489a225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it is stationary implying a differencing term of 1 is appropriate\n",
    "df['diff_log_temp'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b3153",
   "metadata": {},
   "source": [
    "Orders p and q of the ARIMA model can be informed by the autocorrelation and partial autocorrelation figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e814653",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(df['diff_log_temp'].dropna());\n",
    "plot_pacf(df['diff_log_temp'].dropna());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d72ab",
   "metadata": {},
   "source": [
    "The above implies p and q values of 2, as the first significant value is with lags of 2. We therefore, as a first past, attempt to fit an ARIMA(2,2,1) model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b52e3",
   "metadata": {},
   "source": [
    "## Fitting ARIMA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c979d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(df['log_temp'].values, order=(2,2,1))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea715ec",
   "metadata": {},
   "source": [
    "Let's see how well the models fits to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb40682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_temp_predict'] = model_fit.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d1794",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(y = ['log_temp', 'log_temp_predict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb590f3",
   "metadata": {},
   "source": [
    "It appears as though it is fitting OK. However, this isn't informative as it only tells us it fits to data is has seen but doesn't inform us of the models predictive ability. So we split the data set into a training and test set and fit the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd193424",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[:650]\n",
    "test_df = df.iloc[650:]\n",
    "model = ARIMA(train_df['log_temp'].values, order=(2,2,1))\n",
    "model_fit = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d3262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARIMA_predict_plot(model_fit, train_df, test_df, 'log_temp', model_type = 'ARIMA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0b3401",
   "metadata": {},
   "source": [
    "The model is clearly not very good, so let's investigate the accuracy of different ARIMA models. We investigate the following:\n",
    "\n",
    "* There is seasonality in the data, however a simple ARIMA model does not model seasonalitiy.\n",
    "* The ARIMA model orders may not be optimal.\n",
    "\n",
    "Furthermore, there are additional seasonal orders in the SARIMA model, that can be difficult to choose by judgement. Therefore, we using the auto_arima function provided by the pmdarima package that attemptys to identify the optimal SARIMA parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_model = auto_arima(train_df['log_temp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2cf886",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cbd42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARIMA_predict_plot(arima_model, train_df, test_df, 'log_temp', model_type = 'auto_arima')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374dda42",
   "metadata": {},
   "source": [
    "## Exploring the Neural Prophet model.\n",
    "\n",
    "The ARIMA and SARIMA models did not perform well on our of sample data. So let's try one more different class of model. We use the neural prophet package. This is a development of the fbprophet package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485802a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prophet_df = train_df[['log_temp']].reset_index().rename(columns = {'datetime' : 'ds', 'log_temp': 'y'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ccd8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic model\n",
    "n1 = NeuralProphet()\n",
    "model = n1.fit(train_prophet_df, freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f2f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_df = df[['log_temp']].reset_index().rename(columns = {'datetime' : 'ds', 'log_temp': 'y'})\n",
    "forecast = n1.predict(prophet_df)\n",
    "forecast.tail()\n",
    "plot = n1.plot(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1b46ee",
   "metadata": {},
   "source": [
    "The default model is not very good at fitting out of sample. Let's specify the model with things that we know. We know\n",
    "\n",
    "* There is yearly seasonality\n",
    "* There is not daily seasonality\n",
    "* There is not weekly seasonality\n",
    "* Over these two years, there is no growth (would be different if we had more data or were projecting over a longer period of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is not very good, so let's further specify the model \n",
    "#basic model\n",
    "n2 = NeuralProphet(yearly_seasonality = True, daily_seasonality = False, weekly_seasonality = False, growth = 'off')\n",
    "model = n2.fit(train_prophet_df, freq='D')\n",
    "prophet_df = df[['log_temp']].reset_index().rename(columns = {'datetime' : 'ds', 'log_temp': 'y'})\n",
    "forecast = n2.predict(prophet_df)\n",
    "forecast.tail()\n",
    "plot = n2.plot(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a19d5ba",
   "metadata": {},
   "source": [
    "# Model performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d762fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models are auto_arima, n1 and n2\n",
    "train_results_df = train_df[['log_temp']]\n",
    "train_results_df['arima_predict'] = arima_model.predict_in_sample()\n",
    "train_results_df['simple_NP'] = n1.predict(train_prophet_df)['yhat1'].values\n",
    "train_results_df['specified_NP'] = n2.predict(train_prophet_df)['yhat1'].values\n",
    "\n",
    "test_results_df = test_df[['log_temp']]\n",
    "test_prophet_df = test_df[['log_temp']].reset_index().rename(columns = {'datetime' : 'ds', 'log_temp': 'y'})\n",
    "test_results_df['arima_predict'] = arima_model.predict(len(test_df))\n",
    "test_results_df['simple_NP'] = n1.predict(test_prophet_df)['yhat1'].values\n",
    "test_results_df['specified_NP'] = n2.predict(test_prophet_df)['yhat1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e77ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mse(train_results_df['log_temp'], train_results_df['arima_predict']))\n",
    "print(mse(train_results_df['log_temp'], train_results_df['simple_NP']))\n",
    "print(mse(train_results_df['log_temp'], train_results_df['specified_NP']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mse(test_results_df['log_temp'], test_results_df['arima_predict']))\n",
    "print(mse(test_results_df['log_temp'], test_results_df['simple_NP']))\n",
    "print(mse(test_results_df['log_temp'], test_results_df['specified_NP']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c8800d",
   "metadata": {},
   "source": [
    "While the ARIMA model performs better than the neural prophet model on the training set, it doesn't predict out of sample values well.\n",
    "The correctly specified neural prophet model performs much better out of sample.\n",
    "This emphasises the importances of cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a3c486",
   "metadata": {},
   "source": [
    "## Conclusions and suggested next steps for modelling\n",
    "\n",
    "We have shown that the neuralprophet model can predict general trends in average daily temperature about 3 months in the future in California for this given dataset. However, there are is a lot more to be done to this model before it can be verified, or add business value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccebd1b",
   "metadata": {},
   "source": [
    "### Cross - validation\n",
    "\n",
    "Cross-validation should be used to verify the performance of the model. That is, the model should be trained on different sections of the data and then used to predict out-of-sample data to verify that the model does not happen to perform well on this given training and test set.\n",
    "\n",
    "### Use of exogeneous variables\n",
    "\n",
    "Time series prediction assumes that the variable of interest can be explained by its past values. In the case of daily everage temperatures, we know this to not be the case. Therefore, we could use other variables to help us explain temperature. For example, wind speed, precipitation, humidity, temperature in neighbouring areas, etc.\n",
    "\n",
    "### More data\n",
    "\n",
    "We only have data for two years. This is clearly not enough. For the purpose of this project, I only used 2 years of data due to query limits on visual crossing, but inferring yearly seasonality would be difficult with just 2 years of data, though the nerual prophet model did ok.\n",
    "\n",
    "Furthermore, depending on the business, more spatially granular data may be required. These results are for California as a whole. However, a restaurant for example may want to know the weather at each given store. In this case use of a vector auto-regression (VAR) model may be more appropriate.\n",
    "\n",
    "### Use of CO2 levels\n",
    "\n",
    "If we had more data, we would start to see a net upwards trend in temperature. This is correlated with CO2 levels. We could use CO2 as a feature, but also we can use projections of CO2 emissions to help predict future temperature.\n",
    "\n",
    "### Correlate with a variable of interest to the business\n",
    "\n",
    "If the goal of the project is to predict sales of ice-cream, for example, then a history of ice-cream sales would be useful. The correlation of ice-cream sales and past daily average temperatures could be evaluated, and a VAR model could be fit to the two time series.\n",
    "\n",
    "### Exploration of other model choices\n",
    "\n",
    "We have only expored neural prophet and ARIMA-type models in the this exercise. There are other model choices that have proven to be useful in time series analysis questions, particularly once more exogeneous variables and multiple time series are added, such as LSTM and XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd757c52",
   "metadata": {},
   "source": [
    "## Implementation/productionisation concerns\n",
    "\n",
    "### Observing data drift\n",
    "\n",
    "It is not unlikely that daily average temperatures could drift outside of the previously observed ranges. This will need to be monitored, and models re-trained accordingly.\n",
    "\n",
    "If, for-example, the neural prophet model is fitted as above, where there is assumed to be no growth, then, if average temperatures trends upwards, this model will not fit so well anymore. It would be useful to fit a curve to the trend in annual temperatures to assure that the rate of increase in temperatures is not accelarating or deccelerating outside of previously assumed bounds.\n",
    "\n",
    "### Evaluating model performance\n",
    "\n",
    "It is important to ensure that the error on the test set doesn't start to drift upwards. If it does, it may warrant re-training of the model.\n",
    "\n",
    "### Outliers (extreme events)\n",
    "\n",
    "Temperature data, as seen above, is quite noisy. The model did a good job at predicting the general trend in temperatures, however, any univariate model is unlikely to predict sudden cold or warm days. This may affect the usefulness of the model to the businesss, as the extreme events may be more likely to have a business impact (such as on insurance companies)\n",
    "\n",
    "\n",
    "### Other things to add when productionising model\n",
    "\n",
    "* Continuous Integraton (automated unit-tests)\n",
    "* Continuous Deployment (automated package build/dockerisation)\n",
    "* Logging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
